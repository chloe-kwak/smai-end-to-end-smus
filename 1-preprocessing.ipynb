{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b098cad-43ae-4044-a071-e115cadb0501",
   "metadata": {},
   "source": [
    "# [Lab1] 데이터 전처리 with SageMaker Processing\n",
    "\n",
    "이 노트북에서는 SageMaker Processing Job을 사용하여 은행 마케팅 데이터를 전처리합니다.\n",
    "\n",
    "## 주요 내용\n",
    "- SageMaker Processing Job 설정\n",
    "- 데이터 전처리 스크립트 작성\n",
    "- MLflow를 통한 실험 추적\n",
    "- 전처리된 데이터를 S3에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 변수 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bec468a-a0d5-4084-a858-86b35d8d9593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 노트북에서 저장한 변수들 로드\n",
    "%store -r\n",
    "\n",
    "print(\"✅ 저장된 변수들을 로드했습니다.\")\n",
    "print(f\"   - S3 버킷: {bucket}\")\n",
    "print(f\"   - S3 프리픽스: {prefix}\")\n",
    "print(f\"   - 리전: {region}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78db854-61ee-45ce-9a8a-d3e68cff2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 라이브러리 임포트\n",
    "import sagemaker\n",
    "import boto3\n",
    "import mlflow\n",
    "import os\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.sklearn import SKLearn\n",
    "from sagemaker.processing import FrameworkProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker_studio import Project\n",
    "\n",
    "# AWS 세션 초기화\n",
    "boto_session = boto3.Session()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "print(\"✅ 라이브러리 임포트 및 세션 초기화 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26e4246-cc09-450a-8b4b-5636eccea2b5",
   "metadata": {},
   "source": [
    "## 2. 전처리 스크립트 준비\n",
    "\n",
    "SageMaker Processing Job에서 실행될 전처리 스크립트를 작성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-processing-dir",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처리 작업을 위한 디렉토리 생성\n",
    "!mkdir -p processing/requirements\n",
    "\n",
    "print(\"✅ 처리 작업 디렉토리 생성 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requirements-file",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile processing/requirements/requirements.txt\n",
    "mlflow==2.13.2\n",
    "sagemaker-mlflow==0.1.0\n",
    "pandas\n",
    "numpy\n",
    "scikit-learn\n",
    "sagemaker-studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preprocessing-script",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile processing/preprocessing.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Requirements 설치\n",
    "def install_requirements():\n",
    "    requirements_path = '/opt/ml/processing/input/code/requirements.txt'\n",
    "    if os.path.exists(requirements_path):\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-r', requirements_path])\n",
    "            print(\"Requirements installed successfully\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error installing requirements: {e}\")\n",
    "            # 필수 패키지만 개별 설치\n",
    "            essential_packages = ['mlflow==2.13.2', 'sagemaker-mlflow==0.1.0', 'pandas', 'numpy', 'scikit-learn']\n",
    "            for package in essential_packages:\n",
    "                try:\n",
    "                    subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "                except:\n",
    "                    print(f\"Failed to install {package}\")\n",
    "    else:\n",
    "        print(f\"Requirements file not found at {requirements_path}\")\n",
    "\n",
    "# Requirements 설치 실행\n",
    "install_requirements()\n",
    "\n",
    "import mlflow\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# MLflow 설정 - 환경 변수에서 가져오기\n",
    "mlflow_arn = os.getenv('MLFLOW_TRACKING_ARN')\n",
    "mlflow_run_id = os.getenv('MLFLOW_RUN_ID')\n",
    "user_profile_name = os.getenv('USER')\n",
    "domain_id = os.getenv('DOMAIN_ID')\n",
    "\n",
    "print(f\"MLflow ARN: {mlflow_arn}\")\n",
    "print(f\"Run ID: {mlflow_run_id}\")\n",
    "print(f\"User: {user_profile_name}\")\n",
    "print(f\"Domain ID: {domain_id}\")\n",
    "\n",
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--filepath', type=str, default='/opt/ml/processing/input/')\n",
    "    parser.add_argument('--filename', type=str, default='bank-additional-full.csv')\n",
    "    parser.add_argument('--outputpath', type=str, default='/opt/ml/processing/output/')\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "def process_data(args):\n",
    "    \"\"\"데이터 전처리 함수\"\"\"\n",
    "    # 데이터 로드\n",
    "    df = pd.read_csv(os.path.join(args.filepath, args.filename))\n",
    "    print(f\"원본 데이터 크기: {df.shape}\")\n",
    "    \n",
    "    # 데이터 전처리\n",
    "    # 1. 점(.)을 언더스코어(_)로 변경\n",
    "    df = df.replace(regex=r'\\.', value='_')\n",
    "    df = df.replace(regex=r'\\_$', value='')\n",
    "    \n",
    "    # 2. 새로운 특성 추가\n",
    "    df[\"no_previous_contact\"] = (df[\"pdays\"] == 999).astype(int)\n",
    "    df[\"not_working\"] = df[\"job\"].isin([\"student\", \"retired\", \"unemployed\"]).astype(int)\n",
    "    \n",
    "    # 3. 불필요한 컬럼 제거\n",
    "    df = df.drop(['duration', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'], axis=1)\n",
    "    \n",
    "    # 4. 범주형 변수 원-핫 인코딩\n",
    "    df = pd.get_dummies(df)\n",
    "    print(f\"전처리 후 데이터 크기: {df.shape}\")\n",
    "    \n",
    "    # 5. 훈련/검증/테스트 데이터 분할\n",
    "    train_data, validation_data, test_data = np.split(\n",
    "        df.sample(frac=1, random_state=42), \n",
    "        [int(0.7 * len(df)), int(0.9 * len(df))]\n",
    "    )\n",
    "    \n",
    "    print(f\"훈련 데이터: {train_data.shape}\")\n",
    "    print(f\"검증 데이터: {validation_data.shape}\")\n",
    "    print(f\"테스트 데이터: {test_data.shape}\")\n",
    "    \n",
    "    return train_data, validation_data, test_data, df\n",
    "\n",
    "def save_data(train_data, validation_data, test_data, df, output_path):\n",
    "    \"\"\"전처리된 데이터 저장\"\"\"\n",
    "    # 출력 디렉토리 생성\n",
    "    for subdir in ['train', 'validation', 'test', 'baseline']:\n",
    "        os.makedirs(os.path.join(output_path, subdir), exist_ok=True)\n",
    "    \n",
    "    # 훈련 데이터 저장 (타겟 변수를 첫 번째 컬럼으로)\n",
    "    pd.concat([train_data['y_yes'], train_data.drop(['y_yes','y_no'], axis=1)], axis=1).to_csv(\n",
    "        os.path.join(output_path, 'train/train.csv'), index=False, header=False)\n",
    "    \n",
    "    # 검증 데이터 저장\n",
    "    pd.concat([validation_data['y_yes'], validation_data.drop(['y_yes','y_no'], axis=1)], axis=1).to_csv(\n",
    "        os.path.join(output_path, 'validation/validation.csv'), index=False, header=False)\n",
    "    \n",
    "    # 테스트 데이터 저장 (X, y 분리)\n",
    "    test_data['y_yes'].to_csv(os.path.join(output_path, 'test/test_y.csv'), index=False, header=False)\n",
    "    test_data.drop(['y_yes','y_no'], axis=1).to_csv(\n",
    "        os.path.join(output_path, 'test/test_x.csv'), index=False, header=False)\n",
    "    \n",
    "    # 베이스라인 데이터 저장\n",
    "    baseline_path = os.path.join(output_path, 'baseline/baseline.csv')\n",
    "    df.drop(['y_yes','y_no'], axis=1).to_csv(baseline_path, index=False, header=False)\n",
    "    \n",
    "    return baseline_path\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    args, _ = _parse_args()\n",
    "    \n",
    "    # MLflow 설정\n",
    "    if mlflow_arn:\n",
    "        mlflow.set_tracking_uri(mlflow_arn)\n",
    "        mlflow.autolog()\n",
    "        \n",
    "        try:\n",
    "            # 기존 실행 ID 사용 또는 새 실행 생성\n",
    "            if mlflow_run_id:\n",
    "                try:\n",
    "                    client = mlflow.MlflowClient()\n",
    "                    run_info = client.get_run(mlflow_run_id).info\n",
    "                    run_context = mlflow.start_run(run_id=mlflow_run_id)\n",
    "                    print(f\"기존 실행 연결: {mlflow_run_id}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"새 실행 생성: {e}\")\n",
    "                    run_context = mlflow.start_run()\n",
    "            else:\n",
    "                run_context = mlflow.start_run()\n",
    "                \n",
    "            with run_context:\n",
    "                # 데이터 전처리\n",
    "                train_data, validation_data, test_data, df = process_data(args)\n",
    "                \n",
    "                # MLflow에 파라미터 로깅\n",
    "                mlflow.log_params({\n",
    "                    \"train_shape\": train_data.shape,\n",
    "                    \"validate_shape\": validation_data.shape,\n",
    "                    \"test_shape\": test_data.shape,\n",
    "                    \"total_features\": df.shape[1] - 2  # y_yes, y_no 제외\n",
    "                })\n",
    "                \n",
    "                # 태그 설정\n",
    "                mlflow.set_tags({\n",
    "                    'mlflow.user': user_profile_name,\n",
    "                    'mlflow.source.type': 'PROCESSING_JOB',\n",
    "                    'sagemaker.domain_id': domain_id,\n",
    "                    'processing.stage': 'data_preprocessing'\n",
    "                })\n",
    "                \n",
    "                # 데이터 저장\n",
    "                baseline_path = save_data(train_data, validation_data, test_data, df, args.outputpath)\n",
    "                \n",
    "                # MLflow에 아티팩트 로깅\n",
    "                mlflow.log_artifact(local_path=baseline_path)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"MLflow 오류: {e}\")\n",
    "            # MLflow 없이 데이터 처리\n",
    "            train_data, validation_data, test_data, df = process_data(args)\n",
    "            save_data(train_data, validation_data, test_data, df, args.outputpath)\n",
    "    else:\n",
    "        print(\"MLflow ARN이 없습니다. MLflow 없이 처리합니다.\")\n",
    "        train_data, validation_data, test_data, df = process_data(args)\n",
    "        save_data(train_data, validation_data, test_data, df, args.outputpath)\n",
    "    \n",
    "    print(\"✅ 데이터 전처리 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759076db-8655-4849-8688-e6013641e7e8",
   "metadata": {},
   "source": [
    "## 3. 입력 및 출력 경로 설정\n",
    "\n",
    "SageMaker Processing Job에서 사용할 입력 데이터와 출력 경로를 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "input-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터를 S3에 업로드\n",
    "input_source = sess.upload_data(\n",
    "    './bank-additional/bank-additional-full.csv', \n",
    "    bucket=bucket, \n",
    "    key_prefix=f'{prefix}/input_data'\n",
    ")\n",
    "\n",
    "print(f\"✅ 입력 데이터 업로드 완료: {input_source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "output-paths",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 경로 설정\n",
    "train_path = f\"s3://{bucket}/{prefix}/train\"\n",
    "validation_path = f\"s3://{bucket}/{prefix}/validation\"\n",
    "test_path = f\"s3://{bucket}/{prefix}/test\"\n",
    "baseline_path = f\"s3://{bucket}/{prefix}/baseline\"\n",
    "\n",
    "print(\"✅ 출력 경로 설정 완료:\")\n",
    "print(f\"   - 훈련 데이터: {train_path}\")\n",
    "print(f\"   - 검증 데이터: {validation_path}\")\n",
    "print(f\"   - 테스트 데이터: {test_path}\")\n",
    "print(f\"   - 베이스라인: {baseline_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlflow-experiment",
   "metadata": {},
   "source": [
    "## 4. MLflow 실험 시작\n",
    "\n",
    "데이터 전처리 과정을 추적하기 위한 MLflow 실험을 시작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlflow-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로젝트 및 MLflow 설정\n",
    "project = Project()\n",
    "arn = project.mlflow_tracking_server_arn\n",
    "role = project.iam_role\n",
    "\n",
    "mlflow.set_tracking_uri(arn)\n",
    "\n",
    "# 실행 이름 생성\n",
    "run_suffix = strftime('%d-%H-%M-%S', gmtime())\n",
    "run_name = f\"data-preprocessing-{run_suffix}\"\n",
    "\n",
    "# MLflow 실행 시작\n",
    "run_id = mlflow.start_run(\n",
    "    run_name=run_name, \n",
    "    description=\"SageMaker Processing을 사용한 데이터 전처리\"\n",
    ").info.run_id\n",
    "\n",
    "print(f\"✅ MLflow 실행 시작: {run_name}\")\n",
    "print(f\"   - 실행 ID: {run_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processing-job",
   "metadata": {},
   "source": [
    "## 5. SageMaker Processing Job 실행\n",
    "\n",
    "설정된 전처리 스크립트를 SageMaker Processing Job으로 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processor-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 변수 설정\n",
    "domain_id = 'dzd_3kjtx6zt00s3tc'  # 실제 도메인 ID로 변경\n",
    "user_profile_name = os.getenv('USER', 'sagemaker-user')\n",
    "\n",
    "# SageMaker Processing Job 설정\n",
    "sklearn_processor = FrameworkProcessor(\n",
    "    estimator_cls=SKLearn,\n",
    "    framework_version=\"1.2-1\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1, \n",
    "    base_job_name='bank-data-preprocessing',\n",
    "    env={\n",
    "        'MLFLOW_TRACKING_ARN': mlflow_arn,\n",
    "        'MLFLOW_RUN_ID': run_id,\n",
    "        'USER': user_profile_name,\n",
    "        'DOMAIN_ID': domain_id\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✅ Processing Job 프로세서 설정 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processing-inputs-outputs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 및 출력 설정\n",
    "processing_inputs = [\n",
    "    ProcessingInput(\n",
    "        source=input_source, \n",
    "        destination=\"/opt/ml/processing/input\",\n",
    "        s3_input_mode=\"File\",\n",
    "        s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "    )\n",
    "]\n",
    "\n",
    "processing_outputs = [\n",
    "    ProcessingOutput(\n",
    "        output_name=\"train_data\", \n",
    "        source=\"/opt/ml/processing/output/train\",\n",
    "        destination=train_path,\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        output_name=\"validation_data\", \n",
    "        source=\"/opt/ml/processing/output/validation\", \n",
    "        destination=validation_path\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        output_name=\"test_data\", \n",
    "        source=\"/opt/ml/processing/output/test\", \n",
    "        destination=test_path\n",
    "    ),\n",
    "    ProcessingOutput(\n",
    "        output_name=\"baseline_data\", \n",
    "        source=\"/opt/ml/processing/output/baseline\", \n",
    "        destination=baseline_path\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"✅ 입력/출력 설정 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Job 실행\n",
    "print(\"🚀 SageMaker Processing Job 시작...\")\n",
    "\n",
    "sklearn_processor.run(\n",
    "    inputs=processing_inputs,\n",
    "    code='processing/preprocessing.py',\n",
    "    outputs=processing_outputs,\n",
    "    dependencies=['processing/requirements/requirements.txt'],\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(\"✅ Processing Job 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-verification",
   "metadata": {},
   "source": [
    "## 6. 결과 확인\n",
    "\n",
    "전처리된 데이터가 올바르게 생성되었는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "# 훈련 데이터 확인\n",
    "train_data = sess.read_s3_file(\n",
    "    bucket=bucket,\n",
    "    key_prefix=f\"{prefix}/train/train.csv\"\n",
    ")\n",
    "\n",
    "df_train = pd.read_csv(io.StringIO(train_data), header=None)\n",
    "\n",
    "print(\"✅ 전처리 결과 확인:\")\n",
    "print(f\"   - 훈련 데이터 크기: {df_train.shape}\")\n",
    "print(f\"   - 첫 번째 컬럼 (타겟): {df_train[0].value_counts()}\")\n",
    "\n",
    "print(\"\\n📊 훈련 데이터 미리보기:\")\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlflow-finalize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow 실행 완료 및 메타데이터 추가\n",
    "domain_id = \"dzd_3kjtx6zt00s3tc\"  # 실제 도메인 ID\n",
    "project_id = \"6r962gc9cmwjdc\"     # 실제 프로젝트 ID\n",
    "region = \"us-west-2\"              # 실제 리전\n",
    "\n",
    "studio_url = f\"https://{domain_id}.studio.{region}.sagemaker.aws/projects/{project_id}\"\n",
    "\n",
    "mlflow.set_tags({\n",
    "    'mlflow.source.name': studio_url,\n",
    "    'sagemaker.processing_job': sklearn_processor.latest_job.name,\n",
    "    'sagemaker.domain_id': domain_id,\n",
    "    'sagemaker.project_id': project_id,\n",
    "    'processing.status': 'completed'\n",
    "})\n",
    "\n",
    "mlflow.end_run()\n",
    "\n",
    "print(\"✅ MLflow 실행 완료\")\n",
    "print(f\"   - Processing Job: {sklearn_processor.latest_job.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "store-variables",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음 노트북에서 사용할 변수들 저장\n",
    "%store input_source\n",
    "%store train_path\n",
    "%store validation_path\n",
    "%store test_path\n",
    "%store baseline_path\n",
    "\n",
    "print(\"✅ 변수 저장 완료\")\n",
    "print(\"\\n📋 저장된 경로:\")\n",
    "print(f\"   - 입력 데이터: {input_source}\")\n",
    "print(f\"   - 훈련 데이터: {train_path}\")\n",
    "print(f\"   - 검증 데이터: {validation_path}\")\n",
    "print(f\"   - 테스트 데이터: {test_path}\")\n",
    "print(f\"   - 베이스라인: {baseline_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completion",
   "metadata": {},
   "source": [
    "## ✅ 데이터 전처리 완료\n",
    "\n",
    "SageMaker Processing Job을 사용한 데이터 전처리가 성공적으로 완료되었습니다!\n",
    "\n",
    "### 완료된 작업\n",
    "- ✅ 원본 데이터 전처리 (특성 엔지니어링, 원-핫 인코딩)\n",
    "- ✅ 훈련/검증/테스트 데이터 분할\n",
    "- ✅ 전처리된 데이터를 S3에 저장\n",
    "- ✅ MLflow를 통한 실험 추적\n",
    "\n",
    "### 다음 단계\n",
    "이제 `2-training.ipynb` 노트북으로 진행하여 전처리된 데이터로 모델을 훈련할 수 있습니다.\n",
    "\n",
    "### 생성된 데이터\n",
    "- **훈련 데이터**: 모델 훈련용 (70%)\n",
    "- **검증 데이터**: 하이퍼파라미터 튜닝용 (20%)\n",
    "- **테스트 데이터**: 최종 모델 평가용 (10%)\n",
    "- **베이스라인**: 모델 모니터링용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
